#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´ç‰ˆï¼šæŠ“å–æ‰€æœ‰6000éƒ¨ç”µå½±çš„ç£åŠ›é“¾æ¥
åˆ†æ‰¹å¤„ç†ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
"""

import requests
from bs4 import BeautifulSoup
import re
import time
import json
import os
from urllib.parse import urljoin, urlparse, parse_qs
import random

class CompleteDygodScraper:
    def __init__(self):
        self.base_url = "https://www.dygod.net"
        self.search_base = "/e/search/result/index.php"
        self.magnet_links = []
        self.failed_pages = []
        self.processed_movies = set()
        self.session = requests.Session()
        self.stats = {
            'total_pages': 0,
            'processed_pages': 0,
            'total_movies': 0,
            'total_magnets': 0,
            'failed_movies': 0
        }
        
        # Enhanced headers with rotation
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0'
        ]
        
        self.headers_template = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
    
    def get_headers(self):
        """Get randomized headers"""
        headers = self.headers_template.copy()
        headers['User-Agent'] = random.choice(self.user_agents)
        return headers
    
    def estimate_total_pages(self, searchid):
        """Estimate total pages based on content density"""
        print("æ­£åœ¨ä¼°ç®—æ€»é¡µæ•°...")
        
        # Test first few pages to estimate
        total_movies_found = 0
        pages_tested = 0
        
        for test_page in [1, 10, 50, 100]:
            try:
                url = f"{self.base_url}{self.search_base}?page={test_page}&searchid={searchid}"
                response = self.session.get(url, headers=self.get_headers(), timeout=30)
                response.encoding = response.apparent_encoding or 'utf-8'
                
                if response.status_code == 200:
                    movie_links = self.extract_movie_links_from_page(response.content)
                    movies_on_page = len(movie_links)
                    
                    if movies_on_page > 0:
                        total_movies_found += movies_on_page
                        pages_tested += 1
                        print(f"  ç¬¬ {test_page} é¡µ: {movies_on_page} éƒ¨ç”µå½±")
                        
                        # If we found movies, estimate more pages exist
                        if test_page == 1 and movies_on_page >= 20:
                            # Based on user saying ~6000 movies, estimate ~300 pages
                            estimated_pages = 300
                            print(f"  ä¼°ç®—æ€»é¡µæ•°: {estimated_pages} é¡µ (çº¦ {estimated_pages * 20} éƒ¨ç”µå½±)")
                            return estimated_pages
                    else:
                        print(f"  ç¬¬ {test_page} é¡µ: æ— ç”µå½±æ•°æ®")
                        break
                        
            except Exception as e:
                print(f"  æµ‹è¯•ç¬¬ {test_page} é¡µå¤±è´¥: {e}")
                continue
            
            time.sleep(2)  # Be respectful
        
        return 300  # Default estimate
    
    def extract_movie_links_from_page(self, page_content):
        """Extract movie page URLs from search results page"""
        movie_links = []
        soup = BeautifulSoup(page_content, 'html.parser')
        
        # Find movie entries
        movie_entries = soup.find_all('div', class_='co_content8') or soup.find_all('div', class_='co_area2')
        
        if movie_entries:
            for entry in movie_entries:
                links = entry.find_all('a', href=True)
                for link in links:
                    href = link['href']
                    text = link.get_text().strip()
                    
                    # Filter movie links
                    if any(keyword in text.lower() for keyword in ['è“å…‰', 'ä¸­è‹±', 'å›½ç²¤', 'ä¸­å­—']) or \
                       any(keyword in href for keyword in ['/html/', '/html/gndy/', '/html/dyzz/']):
                        
                        if href.startswith('/'):
                            movie_url = f"{self.base_url}{href}"
                        elif href.startswith('http'):
                            movie_url = href
                        else:
                            continue
                            
                        movie_links.append((text, movie_url))
        else:
            # Fallback: find all links and filter
            all_links = soup.find_all('a', href=True)
            for link in all_links:
                href = link['href']
                text = link.get_text().strip()
                
                if any(keyword in text.lower() for keyword in ['è“å…‰', 'ä¸­è‹±', 'å›½ç²¤', 'ä¸­å­—']) or \
                   any(keyword in href for keyword in ['/html/', '/html/gndy/', '/html/dyzz/']):
                    
                    if href.startswith('/'):
                        movie_url = f"{self.base_url}{href}"
                    elif href.startswith('http'):
                        movie_url = href
                    else:
                        continue
                        
                    movie_links.append((text, movie_url))
        
        return movie_links
    
    def extract_magnet_from_movie_page(self, movie_url, movie_title):
        """Extract magnet links from individual movie page"""
        magnet_links = []
        
        try:
            response = self.session.get(movie_url, headers=self.get_headers(), timeout=20)
            response.encoding = response.apparent_encoding or 'utf-8'
            
            if response.status_code == 200:
                # Use regex to find magnet links
                magnet_pattern = re.compile(r'magnet:\?[^"\'<>\\s]+')
                magnets_in_page = magnet_pattern.findall(response.text)
                
                if magnets_in_page:
                    # Decode magnet links
                    from urllib.parse import unquote
                    decoded_magnets = []
                    for magnet in magnets_in_page:
                        try:
                            decoded_magnet = unquote(magnet)
                            decoded_magnets.append(decoded_magnet)
                        except:
                            decoded_magnets.append(magnet)
                    
                    magnet_links.extend(decoded_magnets)
                    print(f"  æ‰¾åˆ° {len(decoded_magnets)} ä¸ªç£åŠ›é“¾æ¥: {movie_title[:30]}...")
                else:
                    # Try to find download links
                    soup = BeautifulSoup(response.content, 'html.parser', from_encoding=response.encoding)
                    download_links = soup.find_all('a', href=True)
                    
                    for dl_link in download_links:
                        href = dl_link['href']
                        text = dl_link.get_text().strip()
                        
                        if 'magnet' in href.lower():
                            magnet_links.append(href)
                        elif any(keyword in text.lower() for keyword in ['è¿…é›·', 'ç£åŠ›', 'magnet']):
                            onclick = dl_link.get('onclick', '')
                            magnet_match = magnet_pattern.search(onclick)
                            if magnet_match:
                                magnet_links.append(magnet_match.group())
                
                # Also check for magnet links in the page content directly
                page_magnets = magnet_pattern.findall(response.text)
                if page_magnets:
                    magnet_links.extend(page_magnets)
                
        except Exception as e:
            print(f"  å¤„ç†ç”µå½±é¡µé¢å¤±è´¥ {movie_title[:30]}...: {e}")
            self.stats['failed_movies'] += 1
        
        return list(set(magnet_links))  # Remove duplicates
    
    def process_search_page(self, page_num, searchid):
        """Process a single search results page"""
        url = f"{self.base_url}{self.search_base}?page={page_num}&searchid={searchid}"
        print(f"æ­£åœ¨å¤„ç†ç¬¬ {page_num} é¡µ: {url}")
        
        try:
            response = self.session.get(url, headers=self.get_headers(), timeout=30)
            response.encoding = response.apparent_encoding or 'utf-8'
            
            if response.status_code == 200:
                movie_links = self.extract_movie_links_from_page(response.content)
                print(f"  æ‰¾åˆ° {len(movie_links)} éƒ¨ç”µå½±")
                
                page_magnets = []
                for i, (movie_title, movie_url) in enumerate(movie_links, 1):
                    if movie_url not in self.processed_movies:
                        print(f"  å¤„ç†ç¬¬ {i}/{len(movie_links)} éƒ¨ç”µå½±: {movie_title[:40]}...")
                        magnets = self.extract_magnet_from_movie_page(movie_url, movie_title)
                        page_magnets.extend(magnets)
                        self.processed_movies.add(movie_url)
                        self.stats['total_movies'] += 1
                        
                        # Random delay to be respectful
                        delay = random.uniform(0.5, 2.0)
                        time.sleep(delay)
                    else:
                        print(f"  è·³è¿‡å·²å¤„ç†çš„ç”µå½±: {movie_title[:40]}...")
                
                self.stats['processed_pages'] += 1
                return page_magnets
            else:
                print(f"  é¡µé¢è®¿é—®å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                self.failed_pages.append(page_num)
                return []
                
        except Exception as e:
            print(f"  å¤„ç†ç¬¬ {page_num} é¡µå¤±è´¥: {e}")
            self.failed_pages.append(page_num)
            return []
    
    def save_progress(self, filename="complete_magnet_links.txt"):
        """Save current progress"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                for i, magnet in enumerate(self.magnet_links, 1):
                    f.write(f"{i}. {magnet}\n\n")
            
            print(f"\nğŸ“ è¿›åº¦å·²ä¿å­˜åˆ° {filename}")
            print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   æ€»é¡µæ•°: {self.stats['total_pages']}")
            print(f"   å·²å¤„ç†é¡µæ•°: {self.stats['processed_pages']}")
            print(f"   æ€»ç”µå½±æ•°: {self.stats['total_movies']}")
            print(f"   æ€»ç£åŠ›é“¾æ¥: {len(self.magnet_links)}")
            print(f"   å¤±è´¥ç”µå½±: {self.stats['failed_movies']}")
            return True
        except Exception as e:
            print(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def save_state(self, filename="complete_scraper_state.json"):
        """Save scraper state for resume capability"""
        state = {
            'processed_movies': list(self.processed_movies),
            'failed_pages': self.failed_pages,
            'magnet_links': self.magnet_links,
            'stats': self.stats,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(state, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ çŠ¶æ€å·²ä¿å­˜åˆ° {filename}")
        except Exception as e:
            print(f"ä¿å­˜çŠ¶æ€å¤±è´¥: {e}")
    
    def load_state(self, filename="complete_scraper_state.json"):
        """Load scraper state for resume"""
        try:
            if os.path.exists(filename):
                with open(filename, 'r', encoding='utf-8') as f:
                    state = json.load(f)
                self.processed_movies = set(state.get('processed_movies', []))
                self.failed_pages = state.get('failed_pages', [])
                self.magnet_links = state.get('magnet_links', [])
                self.stats = state.get('stats', self.stats)
                print(f"ğŸ“‚ çŠ¶æ€å·²åŠ è½½")
                print(f"   å·²å¤„ç†ç”µå½±: {len(self.processed_movies)}")
                print(f"   å·²è·å–ç£åŠ›é“¾æ¥: {len(self.magnet_links)}")
                print(f"   å¤±è´¥é¡µé¢: {len(self.failed_pages)}")
                return True
        except Exception as e:
            print(f"åŠ è½½çŠ¶æ€å¤±è´¥: {e}")
        return False
    
    def scrape_all_pages(self, searchid, start_page=1, end_page=None, batch_size=50, resume=False):
        """Scrape all pages or specified range with batch processing"""
        
        # Load state if resuming
        if resume:
            self.load_state()
        
        # Get total pages if not specified
        if not end_page:
            print("æ­£åœ¨ä¼°ç®—æ€»é¡µæ•°...")
            end_page = self.estimate_total_pages(searchid)
        
        self.stats['total_pages'] = end_page
        
        print(f"ğŸš€ å¼€å§‹æŠ“å–ç¬¬ {start_page} åˆ° {end_page} é¡µï¼Œå…± {end_page - start_page + 1} é¡µ")
        print(f"ğŸ“Š é¢„è®¡æ€»ç”µå½±æ•°: ~{end_page * 20} éƒ¨")
        print(f"ğŸ“¦ æ¯æ‰¹å¤„ç†: {batch_size} é¡µ")
        
        # Process in batches
        current_batch = 0
        for page_num in range(start_page, end_page + 1):
            if page_num in self.failed_pages:
                print(f"â­ï¸  è·³è¿‡ä¹‹å‰å¤±è´¥çš„ç¬¬ {page_num} é¡µ")
                continue
            
            print(f"\n{'='*80}")
            print(f"ğŸ“„ å¤„ç†ç¬¬ {page_num}/{end_page} é¡µ (è¿›åº¦: {((page_num-start_page+1)/(end_page-start_page+1)*100):.1f}%)")
            print(f"{'='*80}")
            
            page_magnets = self.process_search_page(page_num, searchid)
            self.magnet_links.extend(page_magnets)
            self.stats['total_magnets'] = len(self.magnet_links)
            
            # Save progress every batch_size pages
            if page_num % batch_size == 0:
                self.save_progress(f"batch_{current_batch}_magnet_links.txt")
                self.save_state()
                current_batch += 1
                
                print(f"\nğŸ¯ ç¬¬ {current_batch} æ‰¹å®Œæˆï¼")
                print(f"   å·²å¤„ç†: {page_num} é¡µ")
                print(f"   å·²è·å–: {len(self.magnet_links)} ä¸ªç£åŠ›é“¾æ¥")
                print(f"   é¢„è®¡å‰©ä½™æ—¶é—´: {((end_page - page_num) * 0.5 / 60):.1f} å°æ—¶")
            
            # Longer delay between pages to be more respectful
            if page_num < end_page:
                delay = random.uniform(1, 3)
                print(f"â±ï¸  ç­‰å¾… {delay:.1f} ç§’åç»§ç»­...")
                time.sleep(delay)
        
        # Final save
        self.save_progress("complete_all_magnet_links.txt")
        self.save_state()
        
        print(f"\n{'='*80}")
        print(f"ğŸ‰ æŠ“å–å®Œæˆï¼")
        print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        print(f"   æ€»é¡µæ•°: {self.stats['total_pages']}")
        print(f"   å·²å¤„ç†é¡µæ•°: {self.stats['processed_pages']}")
        print(f"   æ€»ç”µå½±æ•°: {self.stats['total_movies']}")
        print(f"   æ€»ç£åŠ›é“¾æ¥: {len(self.magnet_links)}")
        print(f"   å¤±è´¥é¡µé¢: {len(self.failed_pages)}")
        if self.failed_pages:
            print(f"   å¤±è´¥é¡µç : {self.failed_pages}")
        print(f"{'='*80}")

def main():
    scraper = CompleteDygodScraper()
    
    # Configuration for complete scraping
    searchid = "97801"  # From the original URL
    start_page = 1
    end_page = 300  # Estimated 300 pages for ~6000 movies
    batch_size = 20  # Save every 20 pages
    
    print("ğŸ¬ ç”µå½±å¤©å ‚å®Œæ•´ç‰ˆæ‰¹é‡ç£åŠ›é“¾æ¥æŠ“å–å·¥å…·")
    print("=" * 80)
    print(f"ğŸ¯ æœç´¢ID: {searchid}")
    print(f"ğŸ“„ èµ·å§‹é¡µ: {start_page}")
    print(f"ğŸ“„ ç»“æŸé¡µ: {end_page}")
    print(f"ğŸ“¦ æ‰¹å¤„ç†å¤§å°: {batch_size}")
    print(f"ğŸ¥ é¢„è®¡ç”µå½±æ•°: ~{end_page * 20} éƒ¨")
    print("=" * 80)
    print("âš ï¸  è¿™å°†æ˜¯ä¸€ä¸ªé•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡ï¼Œå»ºè®®ï¼š")
    print("   1. ä¿æŒç½‘ç»œè¿æ¥ç¨³å®š")
    print("   2. å®šæœŸæ£€æŸ¥è¿›åº¦æ–‡ä»¶")
    print("   3. å¯ä»¥éšæ—¶ä¸­æ–­ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ ")
    print("=" * 80)
    
    # Ask for confirmation
    response = input("æ˜¯å¦å¼€å§‹å®Œæ•´æŠ“å–ï¼Ÿ(y/N): ")
    if response.lower() == 'y':
        # Start scraping
        scraper.scrape_all_pages(searchid, start_page, end_page, batch_size, resume=True)
    else:
        print("ä»»åŠ¡å·²å–æ¶ˆã€‚æ‚¨å¯ä»¥ä¿®æ”¹é…ç½®åé‡æ–°è¿è¡Œã€‚")

if __name__ == "__main__":
    main()